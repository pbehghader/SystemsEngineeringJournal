\section{Software Quality Understanding by Analysis of Abundant Data (SQUAAD) Toolset}
\label{sec:squaad}

Software developers can prevent failures and disasters and reduce total ownership costs by putting more emphasis on improving software maintainability in their software development process.
One way to improve software maintainability is to produce clean code while changing the software and to continuously assess and monitor code quality while the software is evolving \cite {mexim2015introduction}.

Prior research has been focusing on the analysis of official releases of software to understand how its code quality evolves \cite{PINTO201559,tu2000evolution,ganpati2012comparative,d2008analysing,le2015empirical}.
This approach gives an insight on change in code quality over the major milestones of software development, rather than how code quality evolves during software development process. 
For example, a developer may unknowingly increase the amount of Technical Debt (TD) over a few commits. If that debt is not addressed quickly, it can impose extra cost.
It gets even worse, if they leave the team without paying that debt.
In another example, a developer may simply commit broken code to the repository.
This will break the code for other contributors and slows down the development.
It also causes the unavailability of byte-code for that commit in a post-development analysis.
Since software developers do not ship uncompilable code in official releases, this detail may not be revealed in that coarse-grained analysis. 

Our approach to understand software quality evolution is not limited to study the official releases but to take a step further by analyzing the state of the software after each commit.
For example, our study on the evolution history of  68 open-source software systems shows from one release to the next one, software contains fewer lines of code, classes, code smells, and security vulnerabilities in 8\%, 4\%, 14\%, and 6\% of times.
However, from one revision (produced by a commit) to the next one, these ratios are 18\%($\uparrow$), 3\%($\downarrow$), 17\%($\uparrow$), and 2\%($\downarrow$).
%Analyzing the impact of each commit on software quality can reveal a wealth of information because it holds details of every stage in the software evolution, such as the time of each change and who made the change.
Analyzing the impact of each commit on software quality can reveal a wealth of information because commits carry fine-grained data on every stage of software evolution, such as the author, the time, and the intent (i.e., commit message) of change.


Over the past couple of years, multiple tools and techniques are introduced to study software evolution by commit-level \cite{Tufano2017TSE,Dyer2015TOSEM,diamantopoulos2016qualboa,Tiwari2017msr,PINTO201559,gousios2014lean,Rozenberg2016MSR,Kaur2018,Sokol2013SCAM,Trautsch2017,Trautsch:2016:APE:2901739.2901753}.
Some of them do not run analysis on source code \cite{gousios2014lean,Rozenberg2016MSR}. 
Some of them are designed and implemented to run source code analysis sequentially \cite{Tufano2017TSE,Sokol2013SCAM}.
Consequently, the execution of the study requires strong on-premise resource and takes multiple weeks \cite{Tufano2017TSE}.
There are some mining techniques designed to scale by running static analysis on different revisions of each file in parallel \cite{Dyer2015TOSEM,AlexandruSANER2017,Trautsch2017}.
These techniques are extremely efficient in generating an Abstract Syntax Tree (AST) \cite{AlexandruSANER2017,Dyer2015TOSEM} and calculating file-based quality metrics, such as size, cohesion, and complexity \cite{Trautsch2017}.
They can also aggregate the result of individual files analysis to evaluate the quality of a project.
However, this approach for achieving scale and avoiding redundancy is not suitable to understand software quality evolution by program analysis techniques that
\begin{itemize}
	\item analyze a \textbf{module} considering all of its source code entities and their \textbf{relationship}.
	For example, architecture recovery techniques generate clusters of source code entities by analyzing semantic and/or structure relationships between them \cite{garcia2013comparative,Tzerpos:2000:AAC:832307.837118}.
	It does not suffice to separately analyze the changed files and aggregate the results to recover the architecture of a new revision.
	%	The artifacts generated by these techniques (clusters of entities) associate 
	\item require \textbf{bytecode}.
	Some static and dynamic program analysis techniques depend on the availability of the compiled version (e.g., FindBugs \cite{4602670} and test-coverage \cite{{Malaiya2002ieee}}). 
	A commit may change the version of a dependency. 
	The new dependency is available and the build configuration (e.g., \textit{build.gradle}) is syntactically correct.
	However, the new revisions do not compile as the dependency is not backward compatible \cite{Behnamghader2017qrs}.
	A recent study \cite{AlexandruSANER2017} declares the unavailability of the compiled versions as the main unresolved source for the manual effort in software evolution analysis.
\end{itemize}

In addition, some tools rely on a complex \textbf{environment} to run.
This includes dynamic techniques that need an execution environment and static techniques with specific requirements.
For example, SonarQube \cite{campbellsonarqube} requires deploying its own analysis server, generating a configuration file for each revision, executing the analysis, and fetching the results using SonarQube Api.
%There is also a relative scarcity of commit-level empirical study on using complex program analysis techniques (e.g., building the software, running FindBugs on bytecodes, or running architecture recovery) on different modules, COTS tools with complex environments (e.g., SonarQube), and dynamic analysis techniques (e.g., rendering webpages or test-coverage).
%There is also not much work on a multi-perspective analysis and visualization of software quality evolution from a module/system perspective.

We took steps toward addressing that scarcity by developing Software Quality Understanding by Analysis of Abundant Data (SQUAAD) \cite{cser2018behnamghader}, a comprehensive framework including a cloud-based automated infrastructure accompanied by a data analytics toolset and web interfaces.
%XXX In this paper, we introduce Software Quality Understanding by Analysis of Abundant Data \textbf{(SQUAAD)}, a comprehensive framework including a cloud-based automated infrastructure accompanied by a data analytics and visualization tool-set.
%Our integrated tool-based approach has been documented in multiple research publications empowering their empirical studies \cite{le2015empirical,mahajan2016using,Behnamghader2017,10.1007/978-3-319-62217-0_9}.
%Our approach to conduct large-scale replicable empirical studies on software evolution has been to capitalize on cloud services to full maintainability and technical debt commit histories of large families of open-source software systems available through GitHub.
Our multi-perspective software quality evolution approach assesses
%code and architectural changes. 
%It addresses 
%multiple 
different quality attributes such as software size, code quality, and security.
It utilizes complex program analysis techniques (e.g., byte-code analysis using FindBugs and architecture recovery using ACDC), COTS tools with complex environments (e.g., SonarQube), and dynamic analysis (e.g., unit-test pass-rate).
It enables analysis of the conflicts and synergies between different quality attributes and the difference between developers in terms of their impact on software quality.
Our integrated tool-based approach has been documented in multiple research publications \cite{cser2018behnamghader,Behnamghader2018esem,Behnamghader2017qrs,Behnamghader2017,Alfayez2017stc, Alfayez2018TechDebt} empowering their empirical studies and is used by a major governmental entity.


%Our approach to conduct large-scale replicable empirical studies on software evolution has been to capitalize on cloud services to analyze full maintainability and technical debt commit histories of large families of open-source software systems available through GitHub. SQUAAD automatically
%1)	retrieves a subject system's meta-data (e.g., number of contributors) as well as its commit history from GitHub.
%2)	distributes hundreds of revisions (i.e. official releases and/or revisions created by commits) on multiple cloud instances.
%3)	compiles each revision and runs static/dynamic programming analysis techniques.
%4)	collects and parses the artifacts generated by programming analysis techniques to extract quality attributes.
%5)	runs various statistical analysis on software quality evolution.

SQUAAD is designed to target a module, compile its distinct revisions, and run static/dynamic analysis on it.
Before conducting a large-scale analysis, SQUAAD runs a light-weight mining task on the software's Git repository to determine which commit changes a module (impactful commits \cite{Behnamghader2017qrs}) and the evolutionary relationship between those commits \cite{Behnamghader2018esem}.
Then it automatically
% to automatically 1) retrieve a subject system's meta-data (e.g., number of contributors) as well as its commit history (i.e., Git repository), 2) distribute 
1) distributes hundreds of revisions on multiple cloud instances,
2) compiles each revision using default and/or user defined configurations,
3) provisions the environment and runs static/dynamic programming analysis techniques on each revision,
4) collects the generated artifacts,
5) either parses them to extract quality attributes or compares them to each other to calculate the difference (e.g., by architectural distance metrics), and
% and store them in a relational database,
6) runs various statistical analysis on software quality evolution.

The entire analysis workflow is automated. As soon as the framework is configured for a subject system, we can run the represented analysis on that system and study its evolution.
This full automation makes analysis conducted by SQUAAD replicable and addresses a major threat (i.e., not being replicable) to the external validity of repository mining studies \cite{Trautsch:2016:APE:2901739.2901753}.
We have also developed web interfaces to illustrate the evolution of different quality attributes and the impact of each developer on software quality.

%We assessed the generalizability of my current lead-author paper results by extending the dataset used in those studies either in terms of number of subject systems and organizations, or conducting commit-level analysis instead of release-level.

For example,
one of our recent maintainability trends analysis \cite{Behnamghader2017qrs} involves a total of 19,580 examined revisions from 38 Apache-family systems across a timespan from January 2002 through March 2017, comprising 586 MSLOC.
In this analysis, to obtain software quality, we used three widely-used open-source static analysis tools: FindBugs, PMD, and SonarQube.
We selected a subset of quality attributes related to size (basic), code quality, and security.
We found that on average, 2\% of impactful commits break the compilation.
We qualitatively investigated when, how, and why developers break the compilation and introduced a guideline for preventing developers from committing uncompilable code.
%Our results suggest that the impactful commits have different impact on the software quality metrics.
We found that different quality attributes may change even if the size (i.e., code count) of the software does not change.
We calculated the probability for a metric to change while another one is constant based on the collected data.
%Our results also show that although the security metrics change less frequently, it is crucial to utilize them as they can reveal the introduction of different kinds of security problems.
Our result showed that employing multiple security metrics together can reveal points where security problems are introduced.

%In a follow up work , we extended our original dataset to include revisions committed in 2017/18 of those 38 Apache systems, as well as 30 new open-source systems from Google and Netflix, comprising a total of 37838 distinct revisions and more than 1.5 billion lines of code.
In a follow-up study \cite{Behnamghader2018esem}, we extended our original dataset to include 30 new subject systems from Google and Netflix, as well as revisions committed in 2017 and 2018 of 38 Apache systems of the original dataset.
The extended dataset comprises more than 37k analyzed distinct software revisions and more than 1.5 billion lines of code.
We replicated the analysis mentioned above on the new dataset and extended it to reach the maximum commit compilability (97.7\% for Apache, 99.0\% for Google, and 93.9\% for Netflix) and identify all uncompilable commits that are caused by a developer's fault.
We quantitatively analyzed the reasons for introducing compile errors and proposed a model to detect the uncompilable commits in a post-development analysis based on the meta-date of the commits (i.e., time, message, and committer) and without considering code artifacts.
Then we analyzed all compilable commits to understand the difference between affiliated and external developers of each organization in terms of impacting different quality attributes. 
For example, our analysis shows that although there is no difference between affiliated and external developers in terms of changing the size of the software, external developers of Netflix and affiliated developers of Google have higher ratio of compilable commits.

Tools such as SQUAAD can enable organizations to continuously monitor their sources of TD and remove them quickly during development, rather than pay for them with interest during maintenance. We are creating versions for some government support organizations to aid in their monitoring and prioritization of TD and its removal. 

%XXX Focusing on a module helps better understanding of heterogeneous projects that are developed in different programming languages and/or by different development teams.
%For example Apache Avro\footnote{https://github.com/apache/avro} is implemented in more than 10 programing languages in the same repository.
%Instead of evaluating the whole project, we can focus on its Java implementation by targeting the ``lang/java'' module. 
%This also helps analyzing heterogeneous projects even if the utilized program analysis technique does not support all languages.
%In another example, Apache Parquet-MR\footnote{https://github.com/apache/parquet-mr} contains different Java sub-projects.
%Each sub-project has its own set of developers and reviewers.
%Instead of focusing on the whole project, we can focus on each sub-project to evaluate the performance of each team.
%
%In our large-scale studies on software quality evolution by commit-level, we have identified multiple benefits for focusing on modules:
%\begin{enumerate}
%	\item It provides a more \textbf{accurate} analysis by ignoring irrelevant changes.
%	Apache Santuario \footnote{https://github.com/apache/santuario-java} provides implementation of primary security standards for XML.
%	Its core module (``src/main'') is accompanied by a large number of tests (``src/test'') comprising 30\% of all source files.
%	Consequently, 34\% of commits do not change any code in the core module.
%	Mining the whole repository and including those commits may causes inaccuracy when analyzing the frequency of change in a quality metric.
%	For example, in a study of architectural evolution, introduction of a test case is not necessarily an architectural change.
%	However an automated recovery technique may consider it as a change since a new entity is added.
%	\item It provides a more \textbf{complete} view of evolution for some commit-level mining tasks.
%	In several instances, we have observed that a commit introduces a compilation error in one module and causes the whole software to be uncompilable over a period; however, other modules are compilable and are evolving over that period.
%	For example, in Netflix-Spectator\footnote{https://github.com/netflix/spectator} commit fbeb719, a compile error in a unit test breaks the compilability the software.
%	This causes the unavailability of bytecode over a period which results in missing data points and an incomplete analysis of the software evolution.
%	After focusing on the core module and skipping tests we reached a 100\% compilability and obtained a full view of the evolution of the core module.
%	%	Compilation is necessary if the analysis depends on the availability of byte-code.
%	\item It reduces the \textbf{cost} and \textbf{complexity} of the mining task.
%	Apache Tiles \footnote{https://github.com/apache/tiles} is a templating framework for user interface development.
%	It is designed to be integrated with a variety of other systems.
%	Its core module is accompanied with multiple smaller components (e.g., sdk, agents, and plugins). 
%	Although majority (70\%) of developers change the core module at least once, only 24\% of commits change code in that module.
%	Instead of analyzing all commits to understand the quality (e.g., compilability, test coverage, technical debt, security, etc.) of the core module, we can focus on that smaller subset.
%	\item It helps better understanding of heterogeneous projects that are developed by \textbf{different development teams}.
%	Apache Parquet-MR\footnote{https://github.com/apache/parquet-mr} contains different sub-projects in the same repository.
%	Each sub-project has its own set of developers and reviewers.
%	%	Consequently, only 34\% developers change the core sub-project at least once over the whole development history.
%	Instead of focusing on the whole project, we can focus on each sub-project to evaluate the performance of each team.
%	\item It helps better understanding of \textbf{multi-language} projects.
%	%Some repository mining techniques simply skip a repository if their programming analysis techniques do not support all utilized programming languages \cite{dyer2013boa,Dyer2015TOSEM}.
%	Apache Avro\footnote{https://github.com/apache/avro} is implemented in 10 programming languages in the same repository.
%	Instead of analyzing the whole project and dealing with the issues of analyzing multi-language projects \cite{AlexandruSANER2017,Arbuckle:2011:MMS:2024445.2024461}, we can run analysis on each modules (e.g., ``lang/java'', ``lang/c++'') individually by a proper program analysis technique.
%	Some mining repository techniques only support one programming language \cite{Dyer2015TOSEM} or limit their mining task to one language per project \cite{Trautsch:2016:APE:2901739.2901753,Trautsch2017}.
%	\begin{figure} [h]
%		\centering
%%		\includegraphics[width=\columnwidth]{accumulo_all}
%%		\includegraphics[width=\columnwidth]{accumulo_impactfuls}
%		\caption{The evolution of unused code blocks in the core module of Apache Accumulo using two sets of commits: 1) all 8605 commits (top) and 2) 2010 commits that change the core module (bottom).}
%		\label{fig:accumulo}
%		%	\vspace{-0.5cm}
%	\end{figure}
%	\item It improves the \textbf{understandability} of data visualization when thousands of data points are depicted.
%	Apache Accumulo \footnote{https://github.com/apache/accumulo} is a distributed data storage framework.
%	Over a period of 5 years (10/2011 to 11/2016) 105 developers contribute 8605 commits to its repository.
%	Only 2010 commits by 76 developers change accumulo's core module.
%	Figure \ref{fig:accumulo} shows the evolution of the number of unused code blocks in the core module using all commits (top) and only the ones that change the core module (bottom) over that period.
%	Both graphs show the same evolutionary trend, but the top one contains 6595 (3.3x) more data points that are not relevant to changes in the core module.
%	
%	\item It facilitates \textbf{manual inspection} of individual data points which is a labor intensive task.
%	In Apache CXF-Fediz\footnote{https://github.com/apache/cxf-fediz}, commit b06255a changes the core module and uses a new interface, which causes a compile error.
%	The error is fixed in the next commit changing the core module (44d7340) which is a small commit that comments out the line containing the error and says ``Switching to WSS4J 2.0.0-rc1 + commenting some stuff as a result''.
%	There are four other commits changing code in other modules between these two commits.
%	Instead of manually inspecting all 6 commits to understand why the repository is uncompilable, we can analyze those two commits that change the core module.
%	%	Focusing on the core module makes the inspection more efficient. 
%\end{enumerate}