\subsection{AUTOMATED EMPIRICAL STUDIES ON SOFTWARE QUALITY AND MAINTAINABILITY EVOLUTION}
In order to better understand the software maintainability, as well as conflicts and synergies among software quality attributes, we have conducted a series of empirical studies on open source software systems.

%One of our recent studies concentrates on the analysis of open source software projects to evaluate the relationships between multiple software system characteristics and technical debt and the relationships between software process factors and technical debt \cite{Alfayez2017stc}. In this study, we employed various statistical methods to investigate how technical debt and technical debt density relate to different system characteristics and process characteristics across a representative sample of 91 Apache Java open source projects. From the results of the data analysis on the hypotheses, we can conclude for similar systems that the size of a software system and the software domain it belongs to correlate with its technical debt and technical debt density significantly. While the number of system releases and commits has a significant positive relationship with its technical debt, the results show no significant relationship between system technical debt and the number of its contributors and branches.

Over time, a system's maintenance is increasingly affected by architectural decay, which is caused by careless or unintended architectural changes \cite{medvidovic2010software}.
In a recent work, we conducted a large-scale empirical study of architectural evolution in open source software systems \cite{Behnamghader2017}. This paper presented the largest study of architectural recovery and architectural evolution to date. The study's scope is reflected in the number of subject systems (23), the total number of examined system versions (931), and the total amount of analyzed code (140 MSLOC). Our study corroborated a number of widely held views about the times, frequency, scope, and nature of architectural change. However, the study also resulted in several unexpected findings. The foremost is that a system's versioning scheme is not an accurate indicator of architectural change: major architectural changes may happen between minor system versions. Even more revealing was the observation that a system's architecture may be relatively unstable in the run-up to a release. Another broad conclusion of our study points to the significance  of employing both semantics and structural based architectural perspective to understand software architecture evolution.

Another recent maintainability trends analysis involves a total of 19,580 examined revisions from 38 Apache-family systems across a timespan from January 2002 through March 2017, comprising 586 MSLOC \cite{Behnamghader2017qrs}. In the aforementioned analysis, to obtain software quality, we used three widely-used open-source static analysis tools: FindBugs, PMD, and SonarQube. We selected a subset of quality attributes related to size (basic), code quality, and security. Table \ref{tab:metrics} shows metrics we used in our analysis.


\begin{table}[htbp]
	\centering
	\caption{Quality Metrics}
	\resizebox{\columnwidth}{!}{
		
		% Table generated by Excel2LaTeX from sheet 'Sheet2'
		\begin{tabular}{l|l|l|l|}
			\toprule
			\textbf{Group} & \textbf{Abbr. } & \textbf{Tool} & \multicolumn{1}{l}{\textbf{Description}} \\
			\midrule
			& LC    & SonarQube & Physical Lines excl. Whitespaces/Comments \\
			Basic & FN    & SonarQube & Functions \\
			& CS    & FindBugs & Classes \\
			\midrule
			& CX    & SonarQube & Complexity (Number of Paths) \\
			Code  & SM    & SonarQube & Code Smells \\
			Quality & PD    & PMD   & Empty Code, Naming, Braces, Import Statements,  \\
			&       &       & Coupling, Unused Code, Unnecessary, Design, \\
			&       &       &  Optimization, String and StringBuffer, Code Size \\
			\midrule
			& VL    & SonarQube & Vulnerabilities \\
			Security & SG    & PMD   & Security Guidelines \\
			& FG    & FindBugs & Malicious Code, Security \\
			\bottomrule
		\end{tabular}%	
	}
%	\vspace{-0.3cm}
	\label{tab:metrics}%
\end{table}%


We found that on average, 2\% of the impactful  commits are  not compilable. We investigated when, how, and why developers commit uncompilable code. Our results suggest that the impactful commits have different impact on the software quality metrics. We found that different quality attributes may change even if the code count does not change.
Table \ref{tab:probability} illustrates the probability for a metric to change while another one is constant.
Our results also show that although the security metrics change less frequently, it is crucial to utilize them as they can reveal the introduction of different kinds of security problems.
%For each software system, we identified the module containing most of source codes (main module), and commits which change the main module (impactful commits). We compiled all revisions created by impactful commits and ran static analysis techniques on those revisions. We studied software quality before and after each impactful commit, Commit-Impact Analysis, to understand how the software evolves and how each change impacts its quality.

\begin{table}[htbp]
	\centering
	\caption{$100 * |change(Y) \cap const(X)|/|const(X)|$}
	\resizebox{\columnwidth}{!}{
		
		% Table generated by Excel2LaTeX from sheet 'Sheet1'
		\begin{tabular}{c|ccc|ccc|ccc|}
			\toprule
			\multicolumn{1}{l|}{Const} & \multicolumn{9}{c}{Change} \\
			\midrule
			& \multicolumn{3}{c|}{Basic} & \multicolumn{3}{c|}{Code Quality} & \multicolumn{3}{c}{Security} \\
			\cmidrule{2-10}      & LC    & FN    & CS    & CX    & SM    & PD    & VL    & SG    & \multicolumn{1}{c}{FG} \\
			\midrule
			LC    & -     & 1.2   & 0.4   & 5.5   & 13.3  & 17.0  & 0.7   & 0.2   & 0.6 \\
			FN    & 55.1  & -     & 2.0   & 29.6  & 31.0  & 43.2  & 2.0   & 0.3   & 0.7 \\
			CS    & 65.1  & 24.5  & -     & 45.7  & 38.4  & 54.0  & 3.1   & 0.9   & 1.2 \\
			\midrule
			CX    & 40.1  & 1.8   & 1.6   & -     & 22.8  & 30.2  & 1.2   & 0.3   & 0.7 \\
			SM    & 52.6  & 17.1  & 3.9   & 33.5  & -     & 38.8  & 1.3   & 0.5   & 0.6 \\
			PD    & 37.9  & 6.5   & 1.7   & 17.7  & 16.2  & -     & 1.0   & 0.3   & 0.5 \\
			\midrule
			VL    & 69.1  & 32.8  & 13.8  & 51.5  & 43.7  & 58.8  & -     & 1.6   & 1.4 \\
			SG    & 70.2  & 34.5  & 15.5  & 53.1  & 45.6  & 60.2  & 5.7   & -     & 1.4 \\
			FG    & 70.1  & 34.4  & 15.2  & 53.0  & 45.3  & 60.0  & 4.9   & 0.7   & - \\
			\bottomrule
		\end{tabular}%
	}
	\label{tab:probability}%
	\vspace{-0.3cm}
\end{table}%

A recent extension is a study of the influence of developers on the technical debt in open source software system based on their level of involvement \cite{10.1007/978-3-319-62217-0_9}. In this paper, we investigated whether variations in contributors level of involvement shows variation in their contribution type and its quality. We categorized the contributors of each system into core or peripheral based on the frequency of their commits. We compared their contributions with regards to size (LC) and technical debt (TD). We found that there is no significant difference between core and peripheral developers contributions in terms of the number of commits that change LC, increase LC, change TD, and increase TD in our subject systems. Our study provides strong motivation for volunteer developers to join and contribute to the OSS community even though they are unable to commit much of their time and effort to it.

SQUAAD does not rely on an expensive on-premise infrastructure (e.g., an HPC cluster or a multi-processor server machine). 
We recently conducted a large-scale empirical study consisting in-depth analysis of 35234 distinct revisions of 68 subject systems across three organizations (Apache, Google, and Netflix) to understand the impact of developers on software quality evolution considering their affiliation with an organization.
Table \ref{tab:scale} shows the scale of our study.
The compilation and analysis of all revisions of Google and Netflix projects using PMD, SonarQube, and FindBugs are done by a personal machine and by distributing the analysis over tens of AWS m5.large nodes (2 vCPUs, 8GB Memory). 
The analysis cost \$102 and took a total of 1884 hours. 
Consequently, its replication using 20 cloud instances should take less than a week.

\begin{table}[htbp]
	\centering
	\caption{A Recent Experiment's Scale}
	\resizebox{\columnwidth}{!}{
		% Table generated by Excel2LaTeX from sheet 'MSR Setup'
		\begin{tabular}{l|c|c|c|c|c}
			\textbf{Org.} & \textbf{Time Span} & \textbf{Sys.} & \textbf{Dev.} & \textbf{Rev.} & \textbf{MSLOC} \\
			\midrule
			Netflix & 09/12-12/17 & 12    & 251   & 3683  & 34 \\
			Apache & 01/02-03/17 & 39    & 1102  & 20197 & 576 \\
			Google & 08/08-01/18 & 17    & 402   & 11354 & 753 \\
			\midrule
			\textbf{Total} & 01/02-01/18 & 68    & 1755  & 35234 & 1363 \\
		\end{tabular}%
	}
	\label{tab:scale}%
	\vspace{-0.3cm}
\end{table}%


%We recently conducted a large-scale empirical study consisting in-dept analysis of 35970 distinct revisions of the core module of 68 subject systems across three organizations: Apache, Google, and Netflix.
%We discuss the compilability of projects, and the impact of developers on software quality evolution considering their affiliation with the organization.

Our integrated tool-based approach allows engineers and project managers real time life cycle assessments of software, systems, and system-of-systems. We recently delivered advanced tool assessments tutorials to front line acquisition engineers of a major governmental entity. This led to an in-depth analysis of the quality aspects of an open source software complex for decisions regarding quality, safety, and security "sniffs" and "taints" to assess an acquisition program of an unmanned system.

The analysis conducted by SQUAAD can be applied by any organization wishing to improve its software and its software engineering. At the organization level, managers can determine which of its divisions and project types have better or worse quality; which quality attributes are being achieved poorly or well; and how do these correlate with customer satisfaction and total cost of ownership. At the department or project level, managers can better understand which types of projects or personnel contribute most to quality problems or excellence, and which types of project events correlate with which types of quality increase or decrease.